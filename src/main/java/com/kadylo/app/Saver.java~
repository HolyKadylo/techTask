package com.kadylo.app;

import java.io.Serializable;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.*;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.SparkConf;
import java.util.Iterator;
import java.util.ListIterator;
import java.util.*;
import java.util.HashMap;
import scala.Tuple2;
import java.util.List;
import java.util.ArrayList;
import  java.util.regex.*;
import java.lang.String;
import java.util.Collections;
//import com.google.common.collect;
import java.lang.StringBuilder;
import org.json.*;

//import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
//import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
// http://stackoverflow.com/questions/36364872/creating-a-topic-for-apache-kafka-0-9-using-java?noredirect=1&lq=1
// in order to start Kafka with Java
// import org.I0Itec.zkclient.ZkClient;
// import org.I0Itec.zkclient.ZkConnection;

import kafka.admin.AdminUtils;
import kafka.utils.ZKStringSerializer$;
import kafka.utils.ZkUtils;

// import org.apache.zookeeper.server.quorum.QuorumPeerConfig;
// import org.apache.zookeeper.server.ZooKeeperServer;
// import org.apache.zookeeper.server.ZooKeeperServerMain;
// import org.apache.zookeeper.server.ServerConfig;

import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Properties;
import com.datastax.driver.core.*;

public class Saver implements Runnable{
    //private JavaSparkContext sc;
    
    public void run(){
        System.out.println("=====>Server started");

        System.out.println( "=====>Building receiver Spark context..." );
        SparkConf conf = new SparkConf().setAppName("Saver").setMaster("spark://appliance:7077").set("spark.driver.allowMultipleContexts","true");
        JavaSparkContext sc = new JavaSparkContext(conf);
        
        // https://kafka.apache.org/0101/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html
        //Properties props = new Properties();
        System.out.println( "=====>Loading receiver.properties" );
        final Properties props = new Properties();
        try{
        String propsPath = Saver.class.getProtectionDomain().getCodeSource().getLocation().toURI().getPath();
        propsPath = propsPath.substring(0, propsPath.lastIndexOf('/'));
        InputStream is = null;
        is = new FileInputStream(propsPath + "/extra-resources/receiver.properties");
        props.load(is);  
        System.out.println( "=====>Loaded" );
        } catch (Exception e){
            System.out.println( "=====>Failed: " + e.toString());
            System.exit(-1); 
        }
		
		System.out.println( "=====Creating consumer" );
        KafkaConsumer<String, String> consumer = new KafkaConsumer(props);
		System.out.println( "=====>Consumer created" );
        consumer.subscribe(Arrays.asList("my-topic"));
		System.out.println( "=====>Subscribed to my-topic" );
		
		System.out.println("=====>Building Cassandra cluster");

		Cluster cluster = null;
		System.out.println("=====>Cluster allocated");
		try {
			cluster = Cluster.builder()                                                    // (1)
				.addContactPoint("127.0.0.1")
				.build();
			System.out.println("=====>Cluster built");
			Session session = cluster.connect();   
			System.out.println("=====>Connected");// (2)
			System.out.println("=====>Creating table if needed");// (2)
			
			session.execute("CREATE KEYSPACE IF NOT EXISTS simplex WITH replication " +
                "= {'class':'SimpleStrategy', 'replication_factor':1};");
			session.execute(
                "CREATE TABLE IF NOT EXISTS simplex.countries (" +
                        "name text PRIMARY KEY," +
                        "visits int" +
                        ");");}

			
			
			
			/*ResultSet rs = session.execute("select release_version from system.local");    // (3)
			System.out.println("=====>Query executed");
			Row row = rs.one();
			System.out.println("=====>Got row");
			System.out.println(row.getString("release_version"));   */                     // (4)
		
		
		
			while (true) {
				 ConsumerRecords<String, String> records = consumer.poll(100);
			for (ConsumerRecord<String, String> record : records){
				 //System.out.printf("=====>Offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
				JSONObject rec = new JSONObject(record.value());
				String country = rec.getString("country");
				int visits = rec.getInt("visits");
			 	System.out.println("=====>" + country + " : " + visits);

				session.execute(
				"INSERT INTO simplex.countries (name, visits) " +
				"VALUES (" +
				country + "," +
				visits + ")" +
				";");

				}
			}
}catch (Exception e){
System.out.println("=====>Failed to manage input: " + e.toString());
}
			
		//} finally {
		//	System.out.println("=====>Finally closing cluster");
		//	if (cluster != null) cluster.close();                                          // (5)
		//}	
			
    }
    
    // http://stackoverflow.com/questions/34879414/multiple-sparkcontext-detected-in-the-same-jvm
    Saver(/*SparkContext conf*/){
        //sc = new SparkContext(conf);
    }
}
